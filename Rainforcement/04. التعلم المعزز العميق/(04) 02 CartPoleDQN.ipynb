{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7828,"status":"ok","timestamp":1737395636108,"user":{"displayName":"Ahmed Omar","userId":"12391426561820500567"},"user_tz":-120},"id":"fpPbM2qW5mSC","outputId":"ab13c0f5-d922-4093-977e-e86fa18cd4c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"]}],"source":["!pip install gymnasium"]},{"cell_type":"code","source":["# مكتبة البيئات\n","import gymnasium as gym\n","# مكتبة نمباي\n","import numpy as np\n","# مكتبة تنسر فلو للشبكات العصبية\n","import tensorflow as tf"],"metadata":{"id":"H5t-f6oGvev6","executionInfo":{"status":"ok","timestamp":1737395645554,"user_tz":-120,"elapsed":9457,"user":{"displayName":"Ahmed Omar","userId":"12391426561820500567"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# إنشاء البيئة\n","env = gym.make('CartPole-v1')"],"metadata":{"id":"EXDuGnPqvesT","executionInfo":{"status":"ok","timestamp":1737395645557,"user_tz":-120,"elapsed":34,"user":{"displayName":"Ahmed Omar","userId":"12391426561820500567"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# عدد الأفعال\n","num_actions = env.action_space.n\n","# عدد قيم الحالات\n","num_observations= env.observation_space.shape[0]\n"],"metadata":{"id":"N2FYgVGDvenT","executionInfo":{"status":"ok","timestamp":1737395645558,"user_tz":-120,"elapsed":30,"user":{"displayName":"Ahmed Omar","userId":"12391426561820500567"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# شكل دخل الشبكة العصبية\n","input_shape = (num_observations,)"],"metadata":{"id":"z8TC-ZXhvekD","executionInfo":{"status":"ok","timestamp":1737395645559,"user_tz":-120,"elapsed":29,"user":{"displayName":"Ahmed Omar","userId":"12391426561820500567"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# دالة مخصصة لتعريف نموذج شبكة عصبية\n","def create_dqn_model(input_shape, num_actions):\n","    # بناء شبكة عصبية تحتوي على طبقتين مخفيتين\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Input(shape=input_shape),  # طبقة الدخل\n","        tf.keras.layers.Dense(24, activation='relu'),  # الطبقة الأولى\n","        tf.keras.layers.Dense(24, activation='relu'),  # الطبقة الثانية\n","        # طبقة الخرج والتي عدد الخرج بعدد الأفعال\n","        tf.keras.layers.Dense(num_actions, activation='linear')\n","    ])\n","    return model"],"metadata":{"id":"F5-JIUwvvec7","executionInfo":{"status":"ok","timestamp":1737395645559,"user_tz":-120,"elapsed":28,"user":{"displayName":"Ahmed Omar","userId":"12391426561820500567"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# الشبكة الرئيسية\n","dqn_agent = create_dqn_model(input_shape, num_actions)\n","\n","# الشبكة الهدف\n","target_network = create_dqn_model(input_shape, num_actions)"],"metadata":{"id":"cWjiyfbFwjW8","executionInfo":{"status":"ok","timestamp":1737395649833,"user_tz":-120,"elapsed":4300,"user":{"displayName":"Ahmed Omar","userId":"12391426561820500567"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# المعاملات\n","learning_rate = 0.001  # معدل التعلم\n","discount_factor = 0.95  # معامل الخصم\n","epsilon = 1.0  # معدل الاستكشاف\n","epsilon_decay = 0.9955  # تخفيض الاستكشاف\n","min_epsilon = 0.1  # الحد الأدنى لإبسيلون\n","\n","batch_size = 64  # حجم الدفعة للتدريب"],"metadata":{"id":"g0MOcKYjwjSy","executionInfo":{"status":"ok","timestamp":1737395649833,"user_tz":-120,"elapsed":29,"user":{"displayName":"Ahmed Omar","userId":"12391426561820500567"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from collections import deque\n","# الحجم الأعظمي لذاكرة التجارب\n","buffer_size = 1000000\n","# تهيئة ذاكرة التجارب\n","replay_buffer = deque(maxlen=buffer_size)"],"metadata":{"id":"Bf7ZUaN9wjPL","executionInfo":{"status":"ok","timestamp":1737395649835,"user_tz":-120,"elapsed":27,"user":{"displayName":"Ahmed Omar","userId":"12391426561820500567"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# تعريف دالة الخسارة والمُحسن\n","# دالة الخسارة: الخطأ التربيعي المتوسط\n","loss_fn = tf.keras.losses.MeanSquaredError()\n","# المُحسن: آدم\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"],"metadata":{"id":"FMMdxI9qwjI7","executionInfo":{"status":"ok","timestamp":1737395649835,"user_tz":-120,"elapsed":23,"user":{"displayName":"Ahmed Omar","userId":"12391426561820500567"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# دالة لتحديث أوزان الشبكة الهدف بشكل دوري\n","def update_target_network():\n","    target_network.set_weights(dqn_agent.get_weights())"],"metadata":{"id":"7EufQH0jwsdt","executionInfo":{"status":"ok","timestamp":1737395649836,"user_tz":-120,"elapsed":21,"user":{"displayName":"Ahmed Omar","userId":"12391426561820500567"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# عدد الحلقات التدريبية\n","num_episodes = 2000\n","# بدء التدريب\n","for episode in range(num_episodes):\n","    # إعادة تهيئة البيئة وتهيئة الحالة\n","    state, info = env.reset()\n","    state = state[np.newaxis, :]  # إعادة تشكيل الحالة لتصبح (1, 4)\n","    done = False\n","    episode_reward = 0  # إجمالي المكافآت في الحلقة\n","\n","    while not done:\n","        # اختيار الفعل باستخدام سياسة إبسيلون\n","        if np.random.rand() < epsilon:\n","            # الاستكشاف بشكل عشوائي\n","            action = env.action_space.sample()\n","        else:\n","            # توقع القيم من النموذج\n","            q_values = dqn_agent(state)\n","             # اختيار الفعل الذي يحقق أكبر قيمة\n","            action = np.argmax(q_values.numpy())\n","\n","        # تنفيذ الفعل والحصول على الحالة التالية والمكافأة\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        next_state = next_state[np.newaxis, :]  # إعادة تشكيل الحالة التالية لتصبح (1, 4)\n","        episode_reward += reward  # إضافة المكافأة إلى إجمالي المكافأة في الحلقة\n","\n","        # تخزين التجربة في الذاكرة\n","        replay_buffer.append((state, action, reward, next_state, terminated or truncated))\n","\n","        # إذا كانت الذاكرة تحتوي على تجارب كافية، نبدأ في تدريب النموذج\n","        if len(replay_buffer) > batch_size:\n","            # اختيار دفعة من التجارب العشوائية\n","            batch = np.random.choice(len(replay_buffer), batch_size, replace=False)\n","\n","            # قوائم الحالات و الأفعال والمكافأت والحالات التالية والإتمام\n","            states_batch, actions_batch, rewards_batch, next_states_batch, done_flags_batch = zip(*[replay_buffer[idx] for idx in batch])\n","            # دمج القوائم في مصفوفات واحدة\n","            states_batch = np.vstack(states_batch)\n","            next_states_batch = np.vstack(next_states_batch)\n","\n","            # حساب قيم الخرج من الشبكة الرئيسية\n","            target_q_values = dqn_agent(states_batch).numpy()\n","            # حساب القيم المستهدفة باستخدام الشبكة الهدف\n","            next_q_values = target_network(next_states_batch).numpy()\n","            # إيجاد أكبر القيم\n","            max_next_q_values = np.max(next_q_values, axis=-1)\n","\n","            # استخدام صيغة بل مان لتحديث القيم\n","            for i, action in enumerate(actions_batch):\n","                target_q_values[i, action] = rewards_batch[i] + discount_factor * max_next_q_values[i] * (1 - done_flags_batch[i])\n","\n","            # حساب الخسارة\n","            with tf.GradientTape() as tape:\n","                current_q_values = dqn_agent(states_batch)\n","                loss = loss_fn(current_q_values, target_q_values)\n","\n","            # حساب التدرجات\n","            gradients = tape.gradient(loss, dqn_agent.trainable_variables)\n","            # استخدام المحسن لتحديث الأوزان\n","            optimizer.apply_gradients(zip(gradients, dqn_agent.trainable_variables))\n","\n","        # الانتقال إلى الحالة التالية\n","        state = next_state\n","\n","        # التحقق إذا كانت الحلقة قد انتهت\n","        if terminated or truncated:\n","            done = True\n","\n","    # تقليل احتمال الاستكشاف مع مرور الوقت\n","    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n","\n","    # تحديث أوزان الشبكة الهدف كل 10 حلقات\n","    if (episode + 1) % 10 == 0:\n","        update_target_network()\n","\n","     # طباعة تقدم التدريب كل 100 حلقة\n","    if (episode + 1) % 100 == 0:\n","        print(f\"Episode {episode + 1}: Reward = {episode_reward}, Epsilon = {epsilon:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnsN-MACwsaL","outputId":"98da2d83-0f3e-4672-e7b7-86202dd4730b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 100: Reward = 112.0, Epsilon = 0.637\n","Episode 200: Reward = 237.0, Epsilon = 0.406\n","Episode 300: Reward = 31.0, Epsilon = 0.258\n","Episode 400: Reward = 182.0, Epsilon = 0.165\n","Episode 500: Reward = 17.0, Epsilon = 0.105\n","Episode 600: Reward = 116.0, Epsilon = 0.100\n","Episode 700: Reward = 195.0, Epsilon = 0.100\n","Episode 800: Reward = 111.0, Epsilon = 0.100\n","Episode 900: Reward = 130.0, Epsilon = 0.100\n"]}]},{"cell_type":"code","source":[" # رفع الدرايف\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"wWThhuRawsXK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# مسار مجلد العمل\n","working_folder='/content/drive/MyDrive/RLModels/'\n","# مسار النموذج\n","model_path = working_folder + \"cartpole_dqn_model.h5\""],"metadata":{"id":"OWv3DfhFzzAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifH8oz5aZBYj"},"outputs":[],"source":["# حفظ النموذج المدرب\n","dqn_agent.save(model_path)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}