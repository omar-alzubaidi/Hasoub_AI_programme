# -*- coding: utf-8 -*-
"""Llama_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wx7ndqjMX0eeQPzB6XN4EKrYgjdiznGZ
"""

pip install transformers

pip install huggingface-hub

pip install accelerate

pip install -i https://test.pypi.org/simple/ bitsandbytes

from huggingface_hub import notebook_login

notebook_login()

from transformers import AutoModelForCausalLM,AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-chat-hf', load_in_8bit=True)

tokr = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
prompt = "Hsoub Limited company is a "
toks = tokr(prompt, return_tensors="pt")

toks

tokr.batch_decode(toks['input_ids'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# res = model.generate(**toks.to("cuda"), max_new_tokens=15)
# res

tokr.batch_decode(res)

def gen(p, maxlen=15):
    toks = tokr(p, return_tensors="pt")
    res = model.generate(**toks.to("cuda"), max_new_tokens=maxlen)
    return tokr.batch_decode(res)

gen(prompt, 30)

gen("The sum of the numbers 1 and 25 equal to", 5)

gen("write a python code to sum two numbers", 40)

gen("Write a function in Python that adds two numbers and returns their sum", 60)

gen("Write a function in Python that adds two numbers and returns their sum", 200)

response = gen("Write a function in Python that adds two numbers and returns their sum", 200)

decoded_string = response[0].encode('utf-8').decode('unicode-escape')
print(decoded_string)

gen("ماهي لغة بايثون", 40)

response = gen("اكتب دالة بلغة بايثون تجمع رقمين وتعيد مجموعهما", 600)

text_with_newlines = response[0].replace("\\n", "\n")

print(text_with_newlines)